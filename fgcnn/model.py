# AUTOGENERATED! DO NOT EDIT! File to edit: 02_model.ipynb (unless otherwise specified).

__all__ = ['get_emb_sz', 'ConvPoolRecombine', 'FGCNN']

# Cell
from fastai.tabular.all import *
from .data import *

# Cell
def _fixed_emb_sz(classes, n, k):
    "Pick a fixed embedding size for `n` based on `k`."
    n_cat = len(classes[n])
    sz    = k  # rule of thumb
    return n_cat,sz

def get_emb_sz(to, k):
    "Get default embedding size from `TabularPreprocessor` `proc` or the ones in `sz_dict`"
    return [_fixed_emb_sz(to.classes, n, k) for n in to.cat_names]

# Cell
class ConvPoolRecombine(Module):
    """
    input : ( N, C_in, H, W ), where w represents `k` ( emebdding size )
    output: ( N, new_i, H / 2, W), where `2` represents max pool kernel size which is fixed as `2` for now.
    """

    def __init__(self, ch_in, ch_out, recomb_ch_out, out_wh, h, hp):
        store_attr()

        self.conv = nn.Conv2d(in_channels=ch_in,
                              out_channels=ch_out,
                              kernel_size=(h, 1),
                              padding='same',
                              stride=(1, 1)
                             )

        self.tanh   = nn.Tanh()
        self.pool   = nn.MaxPool2d(kernel_size=(hp, 1))

        self.recomb = nn.Linear(out_wh*ch_out, out_wh*recomb_ch_out)

    def forward(self, x):
        batch_size = x.shape[0]
        embed_size = x.shape[3]

        # output shape: (N, C_out, H, W)
        c = self.conv(x)
        c = self.tanh(c)

        # output shape: (N, C_out, H / hp, W), where hp = `2`
        p = self.pool(c)

        # output shape: (N, H / hp, W, C_out)
        f = p.permute([0, 2, 3, 1]).contiguous()

        # flattening
        # shape: (N, H/2*W*C_out)
        f = f.view(batch_size, -1)

        # recombining
        # shape: (N, H/2, W, C_new)
        r = self.recomb(f).view(batch_size, -1, embed_size, self.recomb_ch_out)

        # shape: (N, c_new*H/hp, W)
        out_r = r.permute([0, 3, 1, 2]).contiguous().view(batch_size, -1, embed_size)

        return p, out_r

# Cell
class FGCNN(Module):
    def __init__(self, emb_szs, conv_kernels, kernels, dense_layers, h, hp):
        """
        emb_szs     : list of tuples representing embedding size e.g. [(4, k), (6, k)]
        conv_kernels: list of convolutional kernels
        kernels     : kernels to be used
        h           : convolutional filter size
        hp          : pooling kernel size
        """

        self.k        = emb_szs[0][1]
        self.n_fields = len(emb_szs)
        self.embeds   = nn.ModuleList([Embedding(ni, nf) for ni,nf in emb_szs])
        self.n_emb    = sum(e.embedding_dim for e in self.embeds)

        self.conv_layers = nn.ModuleList([ConvPoolRecombine(ch_in=1 if i == 0 else conv_kernels[i-1],
                                                            ch_out=conv_kernels[i],
                                                            recomb_ch_out=kernels[i],
                                                            out_wh=int(self.n_fields / (2 ** (i + 1))) * self.k,
                                                            h=h,
                                                            hp=hp
                                                           ) for i in range(len(conv_kernels))
                                         ])

        N = np.sum([int(self.n_fields / (hp ** (i + 1))) for i in range(len(conv_kernels))]) * kernels[0]

        self.lin_in = self.n_fields * self.k + int((N + self.n_fields) * (N + self.n_fields - 1) / 2)

        # MLP classifier
        self.act = nn.ReLU()
        self.mlp = nn.ModuleList([LinBnDrop(n_in=self.lin_in if i == 0  else dense_layers[i-1],
                                            n_out=dense_layers[i],
                                            act=self.act
                                            )
                                          for i in range(len(dense_layers))
                                 ])

        self.final_linear = nn.Linear(in_features=dense_layers[-1], out_features=1, bias=False)
        self.sigmoid      = nn.Sigmoid()

        store_attr()

    def forward(self, x, x_cont):
        bs = x.shape[0]

        if self.n_emb != 0:
            x = [e(x[:,i]).unsqueeze(dim=1) for i,e in enumerate(self.embeds)]
            x = torch.cat(x, 1)

        embed = x.clone()

        input_x = x.view(bs, 1, self.n_fields, self.k)

        p     = input_x
        out_r = []

        for i in range(len(self.conv_layers)):
            p, r = self.conv_layers[i](p)
            out_r.append(r)

        # shape: (bs, C_new * ( H / 2 ^ i), embed_size)
        new_features = torch.cat(out_r, dim=1)

        # combine features
        aug_emb = torch.cat([embed, new_features], dim=1)

        # inner product factorization machine using augmented embedding matrix
        fm = []

        for i in range(aug_emb.shape[1]):
            for j in range(i+1, aug_emb.shape[1]):
                fm.append(torch.sum(torch.mm(aug_emb[:,i,:],
                                             aug_emb[:,j,:].T), dim=1
                                   ).unsqueeze(dim=1))

        fm  = torch.cat(fm, dim=1)

        # flatten
        out = torch.cat([fm, embed.view(bs, -1)], dim=1)

        for i in range(len(self.mlp)):
            out = self.mlp[i](out)

        out = self.final_linear(out)
        out = self.sigmoid(out)

        return out